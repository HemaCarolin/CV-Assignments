{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implement the image stitching for a 360 degree panoramic output. \n",
    "This should function in real-time. You can use any type of features.\n",
    " You can use built-in libraries/tools provided by OpenCV or DepthAI API. \n",
    "You cannot use any built-in function that does output = image_stitch(image1, image2). \n",
    "You are supposed to implement the image_stitch() function\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General definition for stitching two images\n",
    "def stitch_2_imgs(img1, img2, feature):\n",
    "    # Convert images to grayscale\n",
    "    img1_bw = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2_bw = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Initialize feature detector based on user choice\n",
    "    if feature == \"SIFT\":\n",
    "        feat = cv2.SIFT_create() \n",
    "    elif feature == \"ORB\":\n",
    "        feat = cv2.ORB_create()\n",
    "    else:\n",
    "        print(\"Please enter correct feature value.\")\n",
    "        return \n",
    "    \n",
    "    # Detect and compute keypoints and descriptors for both images\n",
    "    kp_img1, desc_img1 = feat.detectAndCompute(img1_bw, None) \n",
    "    kp_img2, desc_img2 = feat.detectAndCompute(img2_bw, None) \n",
    "\n",
    "    # Use Brute Force matcher to find matches between descriptors\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(desc_img2, desc_img1, k=2)\n",
    "\n",
    "    # Filter good matches based on distance ratio\n",
    "    good_points = []\n",
    "    for m, n in matches: \n",
    "        if m.distance < 0.6 * n.distance: \n",
    "            good_points.append(m) \n",
    "\n",
    "    # Extract corresponding points from good matches\n",
    "    query_pts = np.float32([kp_img2[m.queryIdx].pt for m in good_points]).reshape(-1, 1, 2) \n",
    "    train_pts = np.float32([kp_img1[m.trainIdx].pt for m in good_points]).reshape(-1, 1, 2) \n",
    "\n",
    "    # Find homography matrix using RANSAC algorithm\n",
    "    matrix, mask = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0) \n",
    "\n",
    "    # Warp the second image onto the first image based on homography\n",
    "    dst = cv2.warpPerspective(img2, matrix, ((img1.shape[1] + img2.shape[1]), img2.shape[0])) \n",
    "\n",
    "    # Paste the first image onto the stitched image\n",
    "    dst[0:img1.shape[0], 0:img1.shape[1]] = img1\n",
    "\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start defining a pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define a source - left grayscale cameras\n",
    "cam_left = pipeline.createMonoCamera()\n",
    "cam_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "cam_left.setResolution(dai.MonoCameraProperties.SensorResolution.THE_480_P)\n",
    "\n",
    "# Create outputs\n",
    "xout_left = pipeline.createXLinkOut()\n",
    "xout_left.setStreamName('left')\n",
    "cam_left.out.link(xout_left.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RECORDING FOR 5 SECONDS\n",
    "## THE FRAMES ARE BEING STORED IN DIRECTORY \"video_frames\"\n",
    "\n",
    "# Connect and start the pipeline\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "with dai.Device(pipeline) as device:\n",
    "\n",
    "    # Output queue will be used to get the grayscale frames from the output defined above\n",
    "    q = device.getOutputQueue(name=\"left\", maxSize=4, blocking=False)\n",
    "\n",
    "    # Make sure the destination path is present before starting to store the examples\n",
    "    Path(f\"video_frames/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # running loop for 10 secs\n",
    "    ten_secs = time.time() + 10\n",
    "    \n",
    "    while time.time() < ten_secs:\n",
    "        # Blocking call, will wait until a new data has arrived\n",
    "        inSrc = q.get()  \n",
    "        # Data is originally represented as a flat 1D array, it needs to be converted into HxW form\n",
    "        frame = inSrc.getCvFrame()\n",
    "        # Frame is transformed and ready to be shown\n",
    "        cv2.imshow(\"left\", frame)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "        cv2.imwrite(f\"video_frames/{int(time.time() * 10000)}.png\", frame)\n",
    "\n",
    "    cv2.destroyAllWindows()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Science building stitching using SIFT\n",
    "img1 = cv2.imread('image_stitching/sciencecenter1.jpg')\n",
    "img2 = cv2.imread('image_stitching/sciencecenter2.jpg')\n",
    "img3 = cv2.imread('image_stitching/sciencecenter3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# panaroma for science building\n",
    "img2_3 = stitch_2_imgs(img2, img3, \"SIFT\")\n",
    "panorm = stitch_2_imgs(img1, img2_3, \"SIFT\")\n",
    "\n",
    "cv2.imwrite(f\"image_stitching/sciencecent_stiched.jpg\", panorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home building stitching using SIFT\n",
    "\n",
    "imga = cv2.imread('image_stitching/building3.jpg')\n",
    "imgb = cv2.imread('image_stitching/building2.jpg')\n",
    "imgc = cv2.imread('image_stitching/building1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# panaroma for home building\n",
    "imgb_c = stitch_2_imgs(imgb, imgc, \"SIFT\")\n",
    "panorm = stitch_2_imgs(imga, imgb_c, \"SIFT\")\n",
    "\n",
    "cv2.imwrite(f\"image_stitching/building_stiched.jpg\", panorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Science building stitching using Using ORB\n",
    "\n",
    "img1_2 = stitch_2_imgs(img1, img2, \"ORB\")\n",
    "\n",
    "cv2.imwrite(f\"image_stitching/stitch_ORB.png\", img1_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
