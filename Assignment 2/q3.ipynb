{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Consider an image pair from your footage where the images are separated by at least 2 seconds. Also ensure there is at least some overlap of scenes in the two images. \n",
    "Pick a pixel (super-pixel patch as discussed in class) on image 1 and a corresponding pixel ((super-pixel patch as discussed in class)) on image 2 (the pixel on image 2 that corresponds to the same object area on image 1). Compute the SIFT feature for each of these 2 patches. Compute the sum of squared difference (SSD) value between the SIFT vector for these two pixels. Use MATLAB or Python or C++ implementation -- The MATLAB code for SIFT feature extraction and matching can be downloaded from here: https://www.cs.ubc.ca/~lowe/keypoints/ (Please first read the ReadMe document in the folder to find instructions to execute the code).\n",
    "Compute the Homography matrix between these two images using MATLAB or Python or C++ implementation. Compute its inverse.\n",
    "\n",
    "\n",
    "Both a and b sections are implemented in this code.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start defining a pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define a source - left grayscale cameras\n",
    "cam_left = pipeline.createMonoCamera()\n",
    "cam_left.setBoardSocket(dai.CameraBoardSocket.CAM_B)\n",
    "cam_left.setResolution(dai.MonoCameraProperties.SensorResolution.THE_480_P)\n",
    "\n",
    "# Create outputs\n",
    "xout_left = pipeline.createXLinkOut()\n",
    "xout_left.setStreamName('left')\n",
    "cam_left.out.link(xout_left.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RECORDING FOR 10 SECONDS\n",
    "## THE FRAMES ARE BEING STORED IN DIRECTORY \"video_frames\"\n",
    "\n",
    "# Connect and start the pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "\n",
    "    # Output queue will be used to get the grayscale frames from the output defined above\n",
    "    q = device.getOutputQueue(name=\"left\", maxSize=4, blocking=False)\n",
    "\n",
    "    # Make sure the destination path is present before starting to store the examples\n",
    "    Path(f\"video_frames/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # running loop for 10 secs\n",
    "    ten_secs = time.time() + 10\n",
    "    \n",
    "    while time.time() < ten_secs:\n",
    "        # Blocking call, will wait until a new data has arrived\n",
    "        inSrc = q.get()  \n",
    "        # Data is originally represented as a flat 1D array, it needs to be converted into HxW form\n",
    "        frame = inSrc.getCvFrame()\n",
    "        # Frame is transformed and ready to be shown\n",
    "        cv2.imshow(\"left\", frame)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "        cv2.imwrite(f\"video_frames/{int(time.time() * 10000)}.png\", frame)\n",
    "\n",
    "    cv2.destroyAllWindows()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('video_frames/17109832415968.png')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "img2 = cv2.imread('video_frames/17109832440328.png')\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch selection\n",
    "x1, y1 = 100, 100\n",
    "x2, y2 = 150, 150\n",
    "patch_size = 5\n",
    "\n",
    "patch_img1 = img1[y1-patch_size//2:y1+patch_size//2+1, x1-patch_size//2:x1+patch_size//2+1]\n",
    "patch_img2 = img2[y2-patch_size//2:y2+patch_size//2+1, x2-patch_size//2:x2+patch_size//2+1]\n",
    "\n",
    "# Display the selected patches\n",
    "cv2.imshow('Patch in Image 1', patch_img1)\n",
    "cv2.imshow('Patch in Image 2', patch_img2)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the SIFT feature for each of these 2 patches\n",
    "\n",
    "sift = cv2.SIFT_create() \n",
    "\n",
    "kp_img1, desc_img1 = sift.detectAndCompute(img1, None) \n",
    "kp_img2, desc_img2 = sift.detectAndCompute(img2, None) \n",
    "\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(desc_img1, desc_img2, k=2)\n",
    "\n",
    "good_points=[]     \n",
    "for m, n in matches: \n",
    "    if(m.distance < 0.6*n.distance): \n",
    "        good_points.append(m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD value: 7142664.0\n"
     ]
    }
   ],
   "source": [
    "# Filter descriptors based on matches\n",
    "matched_desc_img1 = np.array([desc_img1[m.queryIdx] for m in good_points])\n",
    "matched_desc_img2 = np.array([desc_img2[m.trainIdx] for m in good_points])\n",
    "\n",
    "# Compute the Sum of Squared Difference (SSD) between SIFT descriptors\n",
    "ssd_value = np.sum((matched_desc_img1 - matched_desc_img2) ** 2)\n",
    "print(\"SSD value:\", ssd_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pts = np.float32([kp_img1[m.queryIdx] \n",
    "                .pt for m in good_points]).reshape(-1, 1, 2) \n",
    " \n",
    "train_pts = np.float32([kp_img2[m.trainIdx] \n",
    "                .pt for m in good_points]).reshape(-1, 1, 2) \n",
    " \n",
    "matrix, mask = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0) \n",
    " \n",
    "matches_mask = mask.ravel().tolist() \n",
    "\n",
    "h,w = img1.shape\n",
    " \n",
    "pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    " \n",
    "dst = cv2.perspectiveTransform(pts, matrix)\n",
    " \n",
    "homography = cv2.polylines(img2, [np.int32(dst)], True, (255, 0, 0), 3) \n",
    " \n",
    "cv2.imshow(\"Homography\", homography) \n",
    "cv2.imshow(\"Img\", img1) \n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.02542872e+00, -1.93005712e-02,  5.57186410e+01],\n",
       "       [-1.24831344e-02,  1.15331166e+00, -7.46176095e+01],\n",
       "       [-2.90292532e-04,  1.04463575e-04,  1.00000000e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Homography matrix\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = cv2.drawMatches(img1, kp_img1, img2, kp_img2, good_points, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "cv2.imshow(\"lines\", img3)\n",
    "cv2.waitKey(1000)\n",
    "cv2.imwrite(\"homography.png\", img3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Homography Matrix:\n",
      "[[ 9.60739661e-01  2.07860997e-02 -5.19800992e+01]\n",
      " [ 2.82519837e-02  8.61858698e-01  6.27356736e+01]\n",
      " [ 2.75944245e-04 -8.39987912e-05  9.78356973e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the inverse of the homography matrix\n",
    "inverse_matrix = np.linalg.inv(matrix)\n",
    "print(\"Inverse Homography Matrix:\")\n",
    "print(inverse_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
